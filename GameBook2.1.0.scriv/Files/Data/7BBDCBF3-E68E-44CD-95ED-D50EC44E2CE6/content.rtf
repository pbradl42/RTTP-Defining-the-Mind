{\rtf1\ansi\ansicpg1252\cocoartf1671\cocoasubrtf400
{\fonttbl\f0\fnil\fcharset0 Cochin;\f1\froman\fcharset0 Times-Roman;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\fi360\sl288\slmult1\pardirnatural\partightenfactor0

\f0\fs28 \cf0 ###### What all experimental design has in common\
\
* Intervene in some way to change the value of the IV, measurement of the DV.\
\
* Efforts are made to control extraneous variables.\
\
###### Common threats to internal validity\
\
\pard\pardeftab720\sl216\partightenfactor0

\f1\fs24 \cf0 \expnd0\expndtw0\kerning0
\
* No Control Group\
\
\
* Nonequivalent Control Groups\
\
\
* No adequate measurement of DV  or using an unverified measures\
\
\
* Pre-Test Post-Test Pitfalls:\
\pard\pardeftab720\sl500\partightenfactor0
\cf0 \
	* Maturation---if there is a great deal of time between the pre- and post-test, it is possible than any possible result comes from the maturation of the participants, not the intervention.\
\
	* Testing fatigue---as with standardized testing in high school, participants may become annoyed or bored with the testing, and hence produce poor results.\
\
	* Instrument Decay---if the instrument \'91drifts\'92 or changes over time, internal validity would suffer. This is particularly a problem when the measurement is based on coding or classification.\
\
	* Regression to the mean---with repeated measures, results tend towards the mean.  An initially promising result may just be an outlier, which would prove to be insignificant with repeated measures.\
\
* P-value hacking---Statistical significance is usually measured with p<0.05. 0.05% is one in twenty, which is to say that if a \'91statistically significant\'92 result has a 1 in 20 chance of having been random noise, rather than an actual effect.  Given powerful statistical software we have now, an unscrupulous research could just run his or her data 20 times, or until a statistically significant result appears.\
}